# ================================================================
# 5G O-RAN 1-Cell  ·  3-Part Tariff  ·  2 Slices  ·  SB3 SAC
# ================================================================
# LIGHTWEIGHT CONFIG — 개발/디버깅용 경량 구성
# base: config/default.yaml (v9)
# [M10] 경량 훈련 구성  [Henderson 2018; Haarnoja 2018]
#
# 주요 변경:
#   total_timesteps  1M → 100K   (10x 감소)
#   n_seeds          5  → 1      (5x 감소)
#   N_total        500  → 100    (5x 감소, 매 스텝 연산량 감소)
#   episode_cycles  24  → 12     (2x 감소, 에피소드 360스텝)
#   buffer_size   200K  → 50K    (메모리 절감)
#   n_eval_episodes  5  → 2      (평가 시간 감소)
#   ent_coef_warmup 200K → 20K   (비율 유지)
#
# 예상 훈련 시간: 수 분 (CPU 기준)
# 프로덕션 훈련: config/default.yaml 사용
# ================================================================

# ── §3.1  Time axis ──
# [EP1] episode_cycles 1, episode_mode continuous  [Pardo ICML 2018; Wan arXiv 2025]
time:
  steps_per_cycle: 30
  episode_cycles: 1               # [EP1] 12→1  [Pardo 2018; Wan 2025]
  episode_mode: "continuous"      # [EP1] "continuous" or "episodic" (legacy)

# ── §4  Action bounds (5-D continuous) ──
action:
  F_U_min: 30000.0
  F_U_max: 90000.0
  F_E_min: 40000.0
  F_E_max: 150000.0
  p_over_U_min: 500.0
  p_over_U_max: 5000.0
  p_over_E_min: 200.0
  p_over_E_max: 3000.0
  rho_U_min: 0.05
  rho_U_max: 0.45              # [D1] 0.35→0.45

# ── §5  3-Part tariff allowances ──
tariff:
  Q_U_gb: 5.0
  Q_E_gb: 50.0

# ── §6  Traffic ──
traffic:
  URLLC:
    target_p50_gb_day: 0.15
    target_p90_gb_day: 0.50
    D_min_gb: 0.001
    D_max_gb: 5.0
  eMBB:
    target_p50_gb_day: 1.5
    target_p90_gb_day: 5.0
    D_min_gb: 0.01
    D_max_gb: 50.0

# ── §6b  Demand-price elasticity  [C4] ──
demand_elasticity:
  enabled: true
  epsilon_U: 0.15
  epsilon_E: 0.30
  p_ref_U: 2500.0
  p_ref_E: 1500.0
  floor: 0.5

# ── §7  RAN / PRB capacity ──
# bandwidth_mhz=100, scs_khz=30, prb_total=273 → C_total derivation only
radio:
  C_total_gb_per_step: 400.0       # Derived: 100MHz NR, 273 PRBs [3GPP TS 38.306 R16]
  kappa_U: 1.2

# ── §8  QoS / SLA violation ──
qos:
  alpha_congestion: 10.0
  lambda_U: 500000.0
  lambda_E: 500000.0            # [D3] 200K→500K
  gamma_sla_E: 3.0              # [D3] 2.0→3.0

# ── §9  Cost (Capex excluded) ──
cost:
  c_opex_per_user: 1200.0
  c_energy_per_gb: 50.0
  c_cac_per_join: 80000.0

# ── §10  Market — join / churn ──
market:
  beta0_churn: -5.5
  beta_p_churn: 3.0
  beta_q_churn: 2.0
  beta_sw_churn: 1.5
  beta0_join: -7.0
  beta_p_join: 1.0
  beta_q_join: 1.5
  mode: "stochastic"
  price_norm: 120000.0

# ── §11  CLV ──
clv:
  horizon_months: 24
  discount_rate_monthly: 0.01
  enabled: true

# ── §11b  CLV-aware reward shaping ──
clv_reward_shaping:
  enabled: true
  alpha_retention: 2.0
  warmup_steps: 0              # [EP1] uses _total_steps internally

# ── §13  Population ──
population:
  N_total: 100                  # [M10] 500→100  (5x 감소)
  N_active_init: 40             # [M10] 200→40   (40% 비율 유지)
  frac_urllc: 0.20
  segments:
    names: ["price_sensitive", "balanced", "qos_sensitive"]
    proportions: [0.30, 0.45, 0.25]
    price_sensitivity:   { price_sensitive: 1.5, balanced: 1.0, qos_sensitive: 0.6 }
    qos_sensitivity:     { price_sensitive: 0.5, balanced: 1.0, qos_sensitive: 1.8 }
    switching_cost:      { price_sensitive: 0.3, balanced: 0.5, qos_sensitive: 0.8 }
  # [M13a] Spatial distribution
  spatial:
    cell_radius: 20.0
    segment_radii:
      qos_sensitive: [2.0, 10.0]
      balanced: [3.0, 18.0]
      price_sensitive: [8.0, 20.0]

# ── §12  Training ──
training:
  total_timesteps: 100000       # [M10] 1M→100K  (10x 감소)
  learning_rate: 0.0003
  learning_rate_end: 0.00001
  lr_schedule: "linear"
  batch_size: 256
  buffer_size: 50000            # [M10] 200K→50K  (메모리 절감)
  gamma: 0.995
  tau: 0.005
  ent_coef: "auto"
  ent_coef_init: 0.5
  ent_coef_warmup_steps: 20000  # [M10] 200K→20K  (20% 비율 유지)
  train_freq: 4
  gradient_steps: 4
  eval_freq: 10000
  n_eval_episodes: 10            # [EP1] 2→10; compensate shorter 30-step episodes
  n_seeds: 1                    # [M10] 5→1  (5x 감소)
  curriculum:
    enabled: true
    # [D5] 3-phase curriculum
    phases:
      - fraction: 0.15
        churn_join: false
        lagrangian_boost: 0.0
      - fraction: 0.25
        churn_join: true
        lagrangian_boost: 2.0
      - fraction: 0.60
        churn_join: true
        lagrangian_boost: 1.0

# ── §3.2  Observation  [D5] 24-D ──
observation:
  dim: 24                       # [D5][D6] 24-D (pviol_E EMA + load headroom)
  clip_min: -10.0
  clip_max: 10.0

# ── §15b  Action smoothing ──
action_smoothing:
  enabled: true
  weight: 0.05
  weights: [0.10, 0.05, 0.10, 0.05, 0.03]  # [D1] rho_U 0.01→0.03

# ── §15c  Population-aware reward ──
population_reward:
  enabled: true
  beta_pop: 0.3
  target_ratio: 0.4

# ── §15d  Lagrangian QoS constraint  [M6][D2] ──
lagrangian_qos:
  enabled: true
  pviol_E_threshold: 0.15
  Kp: 0.05                      # [D2] PID gains  [Stooke 2020]
  Ki: 0.005
  Kd: 0.01
  lambda_max: 10.0               # [D2] 5.0→10.0
  update_freq: 200               # [D2] 1000→200

# ── §16  [D1] Admission control (NSACF) ──
admission_control:
  enabled: true
  load_threshold: 0.85
  pviol_ceiling: 0.30
  per_user_load_estimate_E_gb: 1.5
  per_user_load_estimate_U_gb: 0.15

# ── §17  [D2] Hierarchical action timing ──
hierarchical_actions:
  enabled: true

# ── Calibration targets ──
calibration:
  churn_target_monthly: 0.03
