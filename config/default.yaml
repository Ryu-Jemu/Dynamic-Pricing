# ================================================================
# 5G O-RAN 1-Cell  ·  3-Part Tariff  ·  2 Slices  ·  SB3 SAC
# ================================================================
# REVISION 7 — Improvements from training evaluation analysis.
# Change log vs. v6:
#   [R1] total_timesteps 50K → 1M       [Henderson et al. AAAI 2018]
#   [R2] alpha_retention 0.15 → 2.0     [Fader & Hardie 2010; Wiewiora 2003]
#   [R4] Per-dimension action smoothing  [Dalal et al. NeurIPS 2018]
#   [R5] Observation dim 20 → 22        [Dulac-Arnold et al. JMLR 2021]
#   [R6] Population-aware reward term    [Mguni 2019; Zheng 2022]
#   [R8] Entropy coefficient schedule    [Zhou et al. ICLR 2022]
# Prior revisions (v1–v6):
#   [E1] F_E_max 110K → 150K            [KISDI 2023 Korean 5G plans]
#   [E2] total_timesteps 300K → 1M      [Henderson et al. AAAI 2018]
#   [E3] beta_p_churn 1.5 → 3.0         [Kim & Yoon 2004; Ahn 2006]
#   [E4] Observation dim 16 → 20        [Dulac-Arnold et al. JMLR 2021]
#   [E5] episode_cycles 12 → 24         [Gupta JSR 2006: CLV horizon]
#   [E6] CLV-aware reward shaping        [Ng et al. ICML 1999]
#   [E7] Learning rate linear schedule   [Loshchilov & Hutter ICLR 2019]
#   [E8] action_smoothing weight ↑       [Dulac-Arnold 2021 Challenge #7]
#   [E9] EvalCallback best-model ckpt    [Henderson et al. 2018]
#   [C1] C_total 50 → 400               [TS 38.306]
#   [C2] Churn/join logit recalibration  [Ahn 2006, Verbeke 2012]
#   [C3] price_norm re-multiplication fix
#   [C4] Demand-price elasticity         [Nevo et al. 2016]
#   [C5] Action smoothing penalty        [Dulac-Arnold et al. 2021]
#   [F1] Transparent SB3 install/import
#   [F5] CSVLogger SB3 episode-boundary fix
# ================================================================

# ── §1  Scenario ──
scenario:
  name: "5G_ORAN_1Cell_3PT"
  slices: ["URLLC", "eMBB"]

# ── §3.1  Time axis ──
time:
  steps_per_cycle: 30
  episode_cycles: 24

# ── §4  Action bounds (5-D continuous) ──
action:
  F_U_min: 30000.0
  F_U_max: 90000.0
  F_E_min: 40000.0
  F_E_max: 150000.0
  p_over_U_min: 500.0
  p_over_U_max: 5000.0
  p_over_E_min: 200.0
  p_over_E_max: 3000.0
  rho_U_min: 0.05
  rho_U_max: 0.60

# ── §5  3-Part tariff allowances ──
tariff:
  Q_U_gb: 5.0
  Q_E_gb: 50.0

# ── §6  Traffic ──
traffic:
  URLLC:
    target_p50_gb_day: 0.15
    target_p90_gb_day: 0.50
    D_min_gb: 0.001
    D_max_gb: 5.0
  eMBB:
    target_p50_gb_day: 1.5
    target_p90_gb_day: 5.0
    D_min_gb: 0.01
    D_max_gb: 50.0

# ── §6b  Demand-price elasticity  [C4] ──
demand_elasticity:
  enabled: true
  epsilon_U: 0.15
  epsilon_E: 0.30
  p_ref_U: 2500.0
  p_ref_E: 1500.0
  floor: 0.5

# ── §7  RAN / PRB capacity ──
radio:
  bandwidth_mhz: 100
  scs_khz: 30
  prb_total: 273
  C_total_gb_per_step: 400.0
  kappa_U: 1.2

# ── §8  QoS / SLA violation ──
qos:
  alpha_congestion: 10.0
  lambda_U: 500000.0
  lambda_E: 50000.0

# ── §9  Cost (Capex excluded) ──
cost:
  c_opex_per_user: 1200.0
  c_energy_per_gb: 50.0
  c_cac_per_join: 80000.0

# ── §10  Market — join / churn ──
market:
  beta0_churn: -5.5
  beta_p_churn: 3.0
  beta_q_churn: 2.0
  beta_sw_churn: 1.5
  beta0_join: -7.0
  beta_p_join: 1.0
  beta_q_join: 1.5
  mode: "stochastic"
  price_norm: 120000.0

# ── §11  CLV ──
clv:
  horizon_months: 24
  discount_rate_monthly: 0.01
  enabled: true

# ── §11b  CLV-aware reward shaping ──
clv_reward_shaping:
  enabled: true
  alpha_retention: 2.0
  warmup_steps: 100

# ── §13  Population ──
population:
  N_total: 500
  N_active_init: 200
  frac_urllc: 0.20
  segments:
    names: ["price_sensitive", "balanced", "qos_sensitive"]
    proportions: [0.30, 0.45, 0.25]
    price_sensitivity:   { price_sensitive: 1.5, balanced: 1.0, qos_sensitive: 0.6 }
    qos_sensitivity:     { price_sensitive: 0.5, balanced: 1.0, qos_sensitive: 1.8 }
    switching_cost:      { price_sensitive: 0.3, balanced: 0.5, qos_sensitive: 0.8 }

# ── §12  Training ──
training:
  total_timesteps: 10000
  learning_rate: 0.0003
  learning_rate_end: 0.00001
  lr_schedule: "linear"
  batch_size: 256
  buffer_size: 200000
  gamma: 0.995
  tau: 0.005
  ent_coef: "auto"
  ent_coef_init: 0.5
  ent_coef_warmup_steps: 200000
  train_freq: 1
  gradient_steps: 1
  eval_freq: 10000
  n_eval_episodes: 5
  n_seeds: 5
  curriculum:
    enabled: true
    phase1_steps: 200000
    phase2_steps: 800000

# ── §3.2  Observation ──
observation:
  dim: 22
  clip_min: -10.0
  clip_max: 10.0

# ── §15b  Action smoothing ──
action_smoothing:
  enabled: true
  weight: 0.05
  weights: [0.10, 0.05, 0.10, 0.05, 0.01]

# ── §15c  Population-aware reward ──
population_reward:
  enabled: true
  beta_pop: 0.1
  target_ratio: 0.4

# ── Calibration targets ──
calibration:
  demand_tolerance: 0.10
  churn_target_monthly: 0.03
  join_target_monthly: 0.05
