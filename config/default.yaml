# ================================================================
# 5G O-RAN 1-Cell  ·  3-Part Tariff  ·  2 Slices  ·  SB3 SAC
# ================================================================
# REVISION 5 — Enhancements backed by Revision v5 analysis.
# Change log vs. v4:
#   [E1] F_E_max 110K → 150K            [KISDI 2023 Korean 5G plans]
#   [E2] total_timesteps 300K → 1M      [Henderson et al. AAAI 2018]
#   [E3] beta_p_churn 1.5 → 3.0         [Kim & Yoon 2004; Ahn 2006]
#   [E4] Observation dim 16 → 20        [Dulac-Arnold et al. JMLR 2021]
#   [E5] episode_cycles 12 → 24         [Gupta JSR 2006: CLV horizon]
#   [E6] CLV-aware reward shaping        [Ng et al. ICML 1999]
#   [E7] Learning rate linear schedule   [Loshchilov & Hutter ICLR 2019]
#   [E8] action_smoothing weight ↑       [Dulac-Arnold 2021 Challenge #7]
#   [E9] EvalCallback best-model ckpt    [Henderson et al. 2018]
# Prior revisions (v1–v4):
#   [C1] C_total 50 → 400               [TS 38.306]
#   [C2] Churn/join logit recalibration  [Ahn 2006, Verbeke 2012]
#   [C3] price_norm re-multiplication fix
#   [C4] Demand-price elasticity         [Nevo et al. 2016]
#   [C5] Action smoothing penalty        [Dulac-Arnold et al. 2021]
#   [F1] Transparent SB3 install/import
#   [F5] CSVLogger SB3 episode-boundary fix
# ================================================================

# ── §1  Scenario ──
scenario:
  name: "5G_ORAN_1Cell_3PT"
  slices: ["URLLC", "eMBB"]

# ── §3.1  Time axis ──
# [E5] episode_cycles 12 → 24.  Aligns episode length with CLV
# horizon (24 months), so the agent can experience the full
# consequences of user attrition within a single episode.
# [Gupta et al., J. Service Research, 2006]
time:
  steps_per_cycle: 30        # 1 step ≈ 1 day; 30 steps ≈ 1 month
  episode_cycles: 24         # [E5] was 12; now 24 cycles (→ 720 steps)

# ── §4  Action bounds (5-D continuous) ──
# [E1] F_E_max raised from 110K to 150K KRW.
# Rationale: v4 trained agent converged to 99.5% of the 110K ceiling
# (F_E mean = 109,423, CV = 0.3%), indicating the true optimum
# exceeds the bound.  Korean 5G eMBB plans range 55K–130K KRW/month
# [KISDI ICT Industry Policy Report 2023].  150K allows discovery
# of the interior optimum where marginal revenue = marginal churn cost.
action:
  F_U_min: 30000.0           # URLLC base fee min  (KRW/cycle)
  F_U_max: 90000.0
  F_E_min: 40000.0           # eMBB  base fee min
  F_E_max: 150000.0          # [E1] was 110000.0
  p_over_U_min: 500.0        # URLLC overage price min (KRW/GB)
  p_over_U_max: 5000.0
  p_over_E_min: 200.0        # eMBB  overage price min
  p_over_E_max: 3000.0
  rho_U_min: 0.05            # PRB share for URLLC — floor
  rho_U_max: 0.60            # PRB share for URLLC — ceiling

# ── §5  3-Part tariff allowances  [Grubb AER 2009][Nevo 2016][TS 23.503] ──
tariff:
  Q_U_gb: 5.0                # URLLC monthly allowance (GB)
  Q_E_gb: 50.0               # eMBB  monthly allowance (GB)

# ── §6  Traffic  [Nevo 2016 — usage coupled with billing] ──
traffic:
  URLLC:
    target_p50_gb_day: 0.15   # calibration target: median daily URLLC usage
    target_p90_gb_day: 0.50
    D_min_gb: 0.001
    D_max_gb: 5.0
  eMBB:
    target_p50_gb_day: 1.5
    target_p90_gb_day: 5.0
    D_min_gb: 0.01
    D_max_gb: 50.0

# ── §6b  Demand-price elasticity  [C4]  [Nevo et al. 2016] ──
demand_elasticity:
  enabled: true
  epsilon_U: 0.15            # URLLC demand elasticity (low — inelastic)
  epsilon_E: 0.30            # eMBB  demand elasticity (moderate)
  p_ref_U: 2500.0            # URLLC reference overage price (KRW/GB)
  p_ref_E: 1500.0            # eMBB  reference overage price (KRW/GB)
  floor: 0.5                 # minimum demand multiplier (never below 50%)

# ── §7  RAN / PRB capacity  [TS 38.104][TS 38.306][Huang IoT-J 2020] ──
radio:
  bandwidth_mhz: 100         # [TS 38.104] Band n78
  scs_khz: 30                # [TS 38.104]
  prb_total: 273             # [TS 38.104] NRB for 100 MHz @ 30 kHz SCS
  C_total_gb_per_step: 400.0 # [C1] macro cell daily capacity (GB/day)
  kappa_U: 1.2               # URLLC priority factor ≥ 1  [Huang 2020]

# ── §8  QoS / SLA violation ──
qos:
  alpha_congestion: 10.0     # sigmoid steepness
  lambda_U: 500000.0         # SLA penalty weight URLLC (KRW)
  lambda_E: 50000.0          # SLA penalty weight eMBB

# ── §9  Cost (Capex excluded) ──
cost:
  c_opex_per_user: 1200.0    # daily OPEX per active user ≈ 36 000 KRW/mo
  c_energy_per_gb: 50.0      # energy proxy per GB
  c_cac_per_join: 80000.0    # customer acquisition cost  [Kumar & Reinartz 2018]

# ── §10  Market — join / churn ──
# [E3] beta_p_churn raised from 1.5 → 3.0.
# Rationale: v4 agent converged to max fees because marginal churn cost
# was too low relative to marginal revenue.  A stronger price coefficient
# models a more competitive market.
# [Kim & Yoon, Telecom Policy 2004]: Korean churn elasticity −0.3 to −0.8.
# [Ahn et al., Telecom Policy 2006]: 1.5–4.5% monthly churn.
#
# Recalibration with β_p_churn=3.0:
#   Balanced user at mid-price (P_sig≈0.65):
#   churn_logit = −5.5 + 3.0×1.0×0.65 − 2.0×0.95 − 1.5×0.5 = −6.20
#   σ(−6.20) = 0.00203/step → monthly = 5.9%   ✓
#
#   At max-price (P_sig=1.0):
#   churn_logit = −5.5 + 3.0×1.0×1.0 − 2.0×0.95 − 1.5×0.5 = −5.15
#   σ(−5.15) = 0.0058/step → monthly = 15.9%  → strong deterrent
market:
  beta0_churn: -5.5
  beta_p_churn: 3.0          # [E3] was 1.5;  stronger price sensitivity
  beta_q_churn: 2.0          # QoS retention effect
  beta_sw_churn: 1.5         # switching-cost weight
  beta0_join: -7.0
  beta_p_join: 1.0
  beta_q_join: 1.5           # QoS attraction effect
  mode: "stochastic"         # "stochastic" or "expectation"
  price_norm: 120000.0       # [E1] updated: (F_U_max + F_E_max)/2 = (90K+150K)/2 = 120K

# ── §11  CLV  [Gupta JSR 2006] ──
clv:
  horizon_months: 24
  discount_rate_monthly: 0.01
  enabled: true

# ── §11b  CLV-aware reward shaping  [E6]  [Ng et al. ICML 1999] ──
# Adds a retention penalty to the reward function:
#   reward -= alpha_retention × (n_churn / N_active)
# This encodes the lifetime cost of losing customers directly into
# the learning signal, counteracting the myopic pricing tendency.
# [Gupta et al., J. Service Research, 2006]: CLV-optimising firms
# price 15–30% below short-run profit maximisers.
clv_reward_shaping:
  enabled: true
  alpha_retention: 0.15      # penalty weight for churn fraction
  warmup_steps: 100          # no penalty for first N steps (exploration)

# ── §13  Population ──
population:
  N_total: 500
  N_active_init: 200
  frac_urllc: 0.20
  segments:
    names: ["price_sensitive", "balanced", "qos_sensitive"]
    proportions: [0.30, 0.45, 0.25]
    price_sensitivity:   { price_sensitive: 1.5, balanced: 1.0, qos_sensitive: 0.6 }
    qos_sensitivity:     { price_sensitive: 0.5, balanced: 1.0, qos_sensitive: 1.8 }
    switching_cost:      { price_sensitive: 0.3, balanced: 0.5, qos_sensitive: 0.8 }

# ── §12  Training  [Haarnoja 2018 — SAC][SB3][Henderson 2018] ──
# [E2] total_timesteps 300K → 1M.
# Rationale: v4 used 300K steps ≈ 833 episodes.  Henderson et al.
# (AAAI, 2018) recommend ≥2000 episodes for SAC convergence.
# With 720 steps/episode, 1M steps ≈ 1388 episodes.  More stable.
# [E7] Learning rate schedule: linear decay 3e-4 → 1e-5.
# [Loshchilov & Hutter, ICLR 2019]
# [E9] EvalCallback: save best model during training.
training:
  total_timesteps: 1000000   # [E2] was 300K; ~1388 episodes at 720 steps/ep
  learning_rate: 0.0003
  learning_rate_end: 0.00001 # [E7] linear decay target
  lr_schedule: "linear"      # [E7] "linear" or "constant"
  batch_size: 256
  buffer_size: 200000        # [E2] increased to match longer training
  gamma: 0.995               # [E5] was 0.99; longer episodes need higher γ
  tau: 0.005
  ent_coef: "auto"           # [SB3 SAC] automatic entropy tuning
  train_freq: 1
  gradient_steps: 1
  eval_freq: 10000           # [E9] evaluate every 10K steps
  n_eval_episodes: 5
  n_seeds: 5                 # [E9] multi-seed training runs

# ── §3.2  Observation ──
# [E4] dim 16 → 20.  Added 4 derived features:
#   16: cycle_usage_U / (Q_U × N_U)   — URLLC allowance utilisation
#   17: cycle_usage_E / (Q_E × N_E)   — eMBB allowance utilisation
#   18: L_U / C_U                      — URLLC load factor
#   19: L_E / C_E                      — eMBB load factor
# [Dulac-Arnold et al., JMLR 2021, Challenge #1]: task-relevant
# observations reduce sample complexity vs. implicit feature learning.
observation:
  dim: 20                    # [E4] was 16
  clip_min: -10.0
  clip_max: 10.0

# ── §15b  Action smoothing  [C5][E8]  [Dulac-Arnold et al. 2021] ──
# [E8] weight increased 0.01 → 0.05.  v4 smooth penalty was negligible
# (mean 0.00045); stronger weight provides meaningful gradient for
# action stability.
action_smoothing:
  enabled: true
  weight: 0.05               # [E8] was 0.01

# ── Calibration targets ──
calibration:
  demand_tolerance: 0.10
  churn_target_monthly: 0.03
  join_target_monthly: 0.05
