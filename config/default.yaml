# ================================================================
# 5G O-RAN 1-Cell  ·  3-Part Tariff  ·  2 Slices  ·  SB3 SAC
# ================================================================
# REVISION 10.4 (v10.4) — Consolidated from v10 + v5.2/v5.3/v5.5/v5.6 hotfixes
#
# New in v9:
#   [D1] NSACF admission control           [3GPP TS 23.501 §5.2.3; Caballero JSAC 2019]
#   [D2] Hierarchical action timing         [Vezhnevets ICML 2017; Bacon AAAI 2017]
#   [D3] Hard capacity guard                [3GPP TS 23.501 §5.15; Samdanis CommMag 2016]
#   [D4] Segment proportions grounded       [Kim & Yoon Expert Syst 2004; KISDI 2023]
#   [D5] Observation dim 22 → 24           [Dulac-Arnold JMLR 2021]
#   CMDP framing                            [Altman 1999; Berry RAND 1994]
#
# v8 changes:
#   [M1] total_timesteps 10K→1M            [Haarnoja 2018; Henderson 2018]
#   [M2] Curriculum fraction-based          [Narvekar 2020; Bengio 2009]
#   [M3] Convex SLA penalty (λ_E 50K→200K) [Tessler 2019; Paternain 2019]
#   [M4] rho_U_max 0.60→0.35→0.45 [D1]   [Huang IoT-J 2020; 3GPP TS 23.501]
#   [M5] beta_pop 0.1→0.3                 [Mguni 2019; Zheng 2022]
#   [M6] Lagrangian safety layer           [Tessler 2019; Stooke 2020]
#   [M7] train_freq/gradient_steps 1→4    [Fedus 2020]
#
# Prior revisions (v1–v7):
#   [E1–E9] Feature engineering, CLV, multi-seed
#   [C1–C5] Calibration fixes
#   [R1–R8] Training / observation / reward improvements
#
# Architectural references:
#   [REF-1] O-RAN xApp on Near-RT RIC  [O-RAN WG1 OAD 2023; O-RAN WG3 RICARCH 2023]
#   [REF-2] SB3 SAC implementation     [Raffin JMLR 2021]
# ================================================================

# ── §3.1  Time axis ──
# [EP1] episode_cycles 24→1, episode_mode continuous  [Pardo ICML 2018; Wan arXiv 2025]
# 1-cycle episodes with truncated=True; population state persists across resets.
# CLV 24-month horizon is post-hoc (eval.py chains 24 episodes per repeat).
time:
  steps_per_cycle: 30
  episode_cycles: 1               # [EP1] 24→1  [Pardo 2018; Wan 2025]
  episode_mode: "continuous"      # [EP1] "continuous" or "episodic" (legacy)

# ── §4  Action bounds (5-D continuous) ──
# [PR-BOUNDS] Korean 5G market calibration [SKT/KT 2024; ITU DDI 2023]
action:
  F_U_min: 30000.0                # KT 슬림 4GB 30K [KT 2024]
  F_U_max: 90000.0                # SKT 프라임 89K [SKT 2024]
  F_E_min: 35000.0                # [PR-BOUNDS] 40K→35K  [SKT 컴팩트 39K, 다이렉트 27K]
  F_E_max: 110000.0               # [PR-BOUNDS] 150K→110K [SKT 프리미엄 109K; WTP P99]
  p_over_U_min: 500.0
  p_over_U_max: 5000.0
  p_over_E_min: 500.0             # [PR-BOUNDS] 200→500 [Tirole 1988: marginal cost floor]
  p_over_E_max: 3000.0
  rho_U_min: 0.03              # [FIX-M1] 0.05→0.03; C_U=14.4GB (1.2× URLLC load)
  rho_U_max: 0.10              # [WP-3b] 0.12→0.10; C_E≥360GB guaranteed; 17% search space reduction
                                # URLLC load ~12GB, C_U at rho=0.10 = 48GB (4.0× headroom)
                                # [Dulac-Arnold JMLR 2021 Challenge #6; Sciancalepore TNSM 2019]

# ── §5  3-Part tariff allowances ──
# Q_U=5GB: Korean URLLC average  [MSIT 2023; ITU DDI 2023]
# Q_E=50GB: Korean eMBB 5G average  [KCC 2023; KISDI ICT 2023]
tariff:
  Q_U_gb: 5.0
  Q_E_gb: 50.0

# ── §6  Traffic ──
traffic:
  URLLC:
    target_p50_gb_day: 0.15
    target_p90_gb_day: 0.50
    D_min_gb: 0.001
    D_max_gb: 5.0
  eMBB:
    target_p50_gb_day: 1.5
    target_p90_gb_day: 5.0
    D_min_gb: 0.01
    D_max_gb: 50.0

# ── §6b  Demand-price elasticity  [C4] ──
demand_elasticity:
  enabled: true
  epsilon_U: 0.15               # [Nevo et al. Econometrica 2016]
  epsilon_E: 0.30
  p_ref_U: 2500.0
  p_ref_E: 1500.0
  floor: 0.5

# ── §7  RAN / PRB capacity ──
# C_total=400GB: 100MHz NR, 273 PRBs  [3GPP TS 38.306 R16]
# kappa_U=1.2: URLLC scheduling gain  [3GPP TR 38.913 §7.1; Bennis IEEE Comms 2018]
# bandwidth_mhz=100, scs_khz=30, prb_total=273 → C_total derivation only
radio:
  C_total_gb_per_step: 400.0       # Derived: 100MHz NR, 273 PRBs [3GPP TS 38.306 R16]
  kappa_U: 1.2

# ── §8  QoS / SLA violation ──
qos:
  alpha_congestion: 15.0          # [ERR-1] 10→15; sharpen sigmoid for distributional pviol (Jensen's inequality)
                                   # [Caballero JSAC 2019; Sciancalepore TNSM 2019]
  lambda_U: 500000.0
  lambda_E: 500000.0            # [D3] 200K→500K  [Tessler 2019; KCC 2023]
  gamma_sla_E: 3.0              # [F2] 4.0→3.0 revert; γ=4 weakens penalty at pviol_E=0.2 by 5× vs γ=3
                                 # [Bertsekas 1996 §6.3; Paternain TAC 2022 §IV]

# ── §9  Cost (Capex excluded) ──
cost:
  c_opex_per_user: 1200.0
  c_energy_per_gb: 50.0
  c_cac_per_join: 80000.0

# ── §10  Market — join / churn ──
# [PR-1] price_norm REMOVED; per-slice F_s/F_s_max normalization [Train 2009; Anderson et al. 1992]
# [PR-2] Bill shock mechanism [Grubb & Osborne AER 2015]
market:
  beta0_churn: -5.5
  beta_p_churn: 3.0
  beta_q_churn: 2.0
  beta_sw_churn: 1.5
  beta_bill_shock: 1.0            # [PR-2] Bill shock → churn coefficient
  bill_shock_threshold: 1.5       # [PR-2] Triggers when actual_bill/base_fee > 1.5
  beta0_join: -7.0
  beta_p_join: 1.0
  beta_q_join: 1.5
  beta_p_over_join: 0.3           # [PR-4] Overage price dampens joins [Nevo et al. 2016]
  mode: "stochastic"
  # price_norm: 120000.0          # [PR-3] REMOVED → replaced by per-slice normalization

# ── §11  CLV ──
clv:
  horizon_months: 24
  discount_rate_monthly: 0.01
  enabled: true

# ── §11b  CLV-aware reward shaping ──
clv_reward_shaping:
  enabled: true
  alpha_retention: 15.0            # [WP-1b] 2.0→15.0; daily churn 0.125% → penalty 0.019 (~3% of base)  [Fader & Hardie 2010]
  warmup_steps: 0              # [EP1] 100→0; uses _total_steps internally

# ── §13  Population ──
# N_total=500: single gNB suburban cell  [3GPP TR 38.913 §6.1 Table 6.1-1]
# Segment proportions: [D4]
#   30% price-sensitive: consistent with Korean postpaid value-tier share
#     [Kim & Yoon, Expert Syst. Appl. 2004; KISDI ICT Outlook 2023]
#   45% balanced: majority moderate-usage subscribers
#     [Ericsson Mobility Report 2023, Figure 12]
#   25% QoS-sensitive: premium/enterprise tier
#     [GSMA Intelligence 2023; Shy, Rev. Industrial Org. 2002]
# Switching costs: [Shy 2002; Kim et al. Info Econ & Policy 2004]
population:
  N_total: 500
  N_active_init: 200
  frac_urllc: 0.20
  segments:
    names: ["price_sensitive", "balanced", "qos_sensitive"]
    proportions: [0.30, 0.45, 0.25]
    price_sensitivity:   { price_sensitive: 1.5, balanced: 1.0, qos_sensitive: 0.6 }
    qos_sensitivity:     { price_sensitive: 0.5, balanced: 1.0, qos_sensitive: 1.8 }
    switching_cost:      { price_sensitive: 0.3, balanced: 0.5, qos_sensitive: 0.8 }
  # [FIX-S4] Domain Randomization on population initialization
  # [Tobin IROS 2017; Rajeswaran NeurIPS 2017; Henderson AAAI 2018]
  domain_randomization:
    enabled: true
    N_active_init_range: [150, 250]   # [FIX-B2] ±25% wider DR range  [Tobin IROS 2017; Rajeswaran NeurIPS 2017]
    traffic_noise_std: 0.15           # [FIX-B2] Per-step multiplicative traffic perturbation  [Tobin IROS 2017]

# ── §12  Training ──
training:
  total_timesteps: 1000000            # [M1] Full training budget  [Henderson 2018]
  learning_rate: 0.0003
  learning_rate_end: 0.00001
  lr_schedule: "linear"
  batch_size: 512                   # [OPT-D] 256→512 GPU 활용도 향상  [Fedus 2020]
  buffer_size: 200000                # Full replay buffer capacity
  gamma: 0.995
  tau: 0.005
  ent_coef: "auto"
  ent_coef_init: 0.5
  target_entropy: -3.0           # [I-6a] -5(default) → -3.0  [Haarnoja ICML 2018 §5; Zhou ICLR 2022]
  train_freq: 4                 # [M7]  [Fedus 2020]
  gradient_steps: 2             # [OPT-D] 4→2; 총 gradient 계산량 유지 (2×512=4×256)  [Fedus 2020]
  eval_freq: 20000              # [OPT-E] 10K→20K; 평가 간격 2배
  n_eval_episodes: 20            # [EP1] 짧은 에피소드 보상 분산 보상; 20 episodes
  n_seeds: 5                        # [E9] Multi-seed for statistical significance  [Henderson 2018]
  parallel_seeds: true          # [OPT-A] 다중 seed 병렬 실행  [Henderson 2018]
  max_parallel: 5                # [OPT-A] 최대 동시 프로세스 수
  curriculum:
    enabled: true
    # [D5] 3-phase curriculum  [Narvekar 2020; Bengio 2009; Achiam 2017]
    # [WP-2b] Phase-specific PID gains  [Stooke 2020 §3.2; Narvekar 2020]
    phases:
      - fraction: 0.10          # Phase 1: pricing isolation (no market)
        churn_join: false
        lagrangian_boost: 0.0
        pid_gains: {Kp: 0.0, Ki: 0.0, Kd: 0.0}          # Lagrangian disabled
      - fraction: 0.35          # Phase 2: QoS-focused learning
        churn_join: true
        lagrangian_boost: 2.0
        boost_decay_steps: 10000
        pid_gains: {Kp: 0.08, Ki: 0.005, Kd: 0.02}      # High Kp for fast constraint response
      - fraction: 0.55          # Phase 3: full optimization
        churn_join: true
        lagrangian_boost: 1.0
        boost_decay_steps: 5000
        pid_gains: {Kp: 0.05, Ki: 0.015, Kd: 0.01}      # Higher Ki for steady-state error elimination
  # [OPT-C] Early stopping  [Prechelt 2002; Henderson 2018]
  early_stopping:
    enabled: true
    patience: 15               # [V11-6] 10→15  [Henderson AAAI 2018]
    min_timesteps: auto         # auto = 0.75 × total_timesteps; Phase 3 충분 학습 보장  [Prechelt 2002]
    min_improvement: 0.01      # 1% 상대 개선 임계값

# ── §3.2  Observation  [ME-1] 23-D ──
# [ME-1] 24D→23D: removed obs[1] (inactive fraction = 1 - obs[0], linearly dependent)
# [Dulac-Arnold JMLR 2021] — minimize observation dimensionality for sample efficiency
observation:
  dim: 23                       # [ME-1] 24→23; [D5][D6] pviol_E EMA + load headroom
  clip_min: -10.0
  clip_max: 10.0

# ── §15b  Action smoothing ──
action_smoothing:
  enabled: true
  weight: 0.05
  weights: [0.10, 0.05, 0.10, 0.05, 0.30]   # [WP-1b] rho_U: 0.05→0.30; C_E downstream impact proportional  [Dalal NeurIPS 2018 §4.1]

# ── §15c  Population-aware reward ──
population_reward:
  enabled: true
  beta_pop: 2.0                 # [WP-1b] 1.0→2.0; quadratic 3× at -5% → penalty 0.015 (~2% of base)  [Mguni 2019 §4.2]
  target_ratio: 0.37            # [ERR-5] 0.4→0.37; capacity-aligned (185 users ≈ 80% C_E)  [Boyd 2004]
  quadratic: true               # [V11-8] Asymmetric quadratic  [Zheng 2022; Mguni 2019]

# ── §15d  Lagrangian QoS constraint  [M6][D2] ──
# [D2] PID control replaces simple dual ascent  [Stooke ICLR 2020]
lagrangian_qos:
  enabled: true
  pviol_E_threshold: 0.08       # [FIX-S2] 0.10→0.08; 20% 추가 마진 for train-eval shift  [Tobin IROS 2017; Rajeswaran NeurIPS 2017]
  Kp: 0.05                      # [D2] Proportional gain  [Stooke 2020]
  Ki: 0.01                      # [ERR-3] 0.02→0.01; slower integral with tighter λ range [0.5, 5.0]  [Stooke 2020; Boyd 2004]
  Kd: 0.01                      # [D2] Derivative gain
  lambda_max: 15.0               # [WP-1a] 5.0→15.0; dual headroom for λ* convergence  [Boyd 2004 §5.5.3; Stooke 2020]
  lambda_min: 0.5                # [FIX-M3] 0.1→0.5; penalty at pviol=0.2: 0.5×0.1=0.05 (2.8% of reward range)  [Paternain CDC 2019; TAC 2022]
  update_freq: 200               # [D2] 1000→200  [Stooke 2020]
  eval_lambda_floor: 2.0         # [FIX-S5] 1.0→2.0; eval 시 최소 QoS 인센티브 강화  [Tessler 2019 §5; Tobin IROS 2017]

# ── §16  [D1] Admission control (NSACF) ──
# [3GPP TS 23.501 §5.2.3; Caballero et al. IEEE JSAC 2019]
# Gating: reject new joins if projected load > threshold or pviol > ceiling
admission_control:
  enabled: true
  load_threshold: 0.80          # [ERR-4] 0.92→0.80; proactive admission gating  [Caballero JSAC 2019 §IV-B; 3GPP TS 23.501 §5.2.3]
  pviol_ceiling: 0.25           # [ERR-4] 0.50→0.25; tighter QoS gate  [Sciancalepore TNSM 2019]
  per_user_load_estimate_E_gb: 1.5    # eMBB p50 daily traffic
  per_user_load_estimate_U_gb: 0.15   # URLLC p50 daily traffic

# ── §17  [D2] Hierarchical action timing ──
# [Vezhnevets et al. ICML 2017; Bacon et al. AAAI 2017]
# Pricing locked at cycle start; rho_U updates every step
hierarchical_actions:
  enabled: true

# ── §18  [I-3b] eMBB Capacity Guard ──
# Soft penalty when L_E/C_E exceeds threshold
# [3GPP TS 23.501 §5.15.7; Samdanis CommMag 2016; Huang IoT-J 2020]
capacity_guard:
  enabled: true
  embb_load_ratio_max: 0.78      # [ADD-1] 0.85→0.78; activates before AC threshold  [3GPP TS 23.501 §5.15.7; Wiewiora ICML 2003]
  penalty_scale: 4.0             # [WP-1b] 2.0→4.0; anticipatory capacity penalty  [Huang IoT-J 2020; Samdanis CommMag 2016]

# ── §19  [I-5a] SLA Awareness Penalty ──
# Additional reward shaping when SLA penalty exceeds revenue threshold
# [Wiewiora ICML 2003; Ng ICML 1999]
sla_awareness:
  enabled: true
  revenue_ratio_threshold: 0.05  # Trigger when sla_penalty/revenue > 5%
  penalty_scale: 0.5             # [WP-1b] 0.1→0.5; amplify SLA cost signal attenuated by log-transform  [Wiewiora ICML 2003]

# ── §20a  [WTP-REF] Reference-Dependent WTP ──
# 가격 상승 시 지불의도 감소: 참조가격 형성 + 비대칭 손실 회피
# [Koszegi & Rabin QJE 2006; Kahneman & Tversky Econometrica 1979]
wtp_reference:
  enabled: true
  # 참조가격 EMA 적응률 (빌링 사이클 경계에서 업데이트)
  # [Koszegi & Rabin QJE 2006 §III.A — personal equilibrium adaptation]
  alpha_ref: 0.3
  # 손실 회피 계수: 가격 상승(loss) 가중 = lambda_loss × 가격 하락(gain)
  # [Tversky & Kahneman J.Risk&Uncertainty 1992: λ ∈ [2.0, 2.5]; 보수적 하한]
  lambda_loss: 2.0
  # 참조의존 이탈 계수 (10% F 인상 → +0.15 추가 logit)
  # [Bolton, Warlop & Alba JCR 2003: 가격 불공정 → 2-3× churn at 30% increase]
  beta_ref_churn: 1.5
  # 참조의존 가입 억제 (비가입자: 약한 참조 앵커)
  # [Train 2009: 잠재 가입자는 경험 가격이 아닌 사전 신념 기반]
  beta_ref_join: 0.5
  # 기본료 수요 탄력성 [Nevo et al. Econometrica 2016]
  base_fee_elasticity_enabled: true
  epsilon_F_U: 0.10              # URLLC: 미션 크리티컬, 비탄력적
  epsilon_F_E: 0.20              # eMBB: 중간 탄력성 [Nevo 2016: -0.4~-0.6]
  floor_F: 0.7                   # 기본료 수요 하한 (초과요금 floor=0.5보다 높음)

# ── §20b  Evaluation settings ──
eval:
  action_ema_alpha: 0.7          # [FIX-B1] Eval-time action EMA smoothing  [Dulac-Arnold JMLR 2021 §4.2]

# ── Calibration targets ──
calibration:
  churn_target_monthly: 0.03