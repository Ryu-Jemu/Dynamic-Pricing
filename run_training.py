#!/usr/bin/env python3
"""
5G O-RAN Network Slicing - Optimized CMDP-SAC Training
======================================================

ë§¥ë¶ MPS ê°€ì†, tqdm ì§„í–‰ë¥  í‘œì‹œ, ê³„ì‚°ëŸ‰ ìµœì í™” ì ìš©

í•œ ë²ˆì— ì‹¤í–‰:
    pip install -r requirements.txt && python run_training.py -t 100000 -f

Author: Research Team
Date: 2026-01
"""

import os
import sys
import time
import json
import argparse
import warnings
from datetime import datetime
from typing import Dict, Tuple

import numpy as np

# ê²½ê³  ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

# í”„ë¡œì íŠ¸ ê²½ë¡œ
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_ROOT)


def get_device() -> str:
    """ë§¥ë¶ MPS ê°€ì† ìë™ ê°ì§€"""
    try:
        import torch
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            # MPS í…ŒìŠ¤íŠ¸
            try:
                x = torch.zeros(1, device='mps')
                del x
                return "mps"
            except:
                pass
        if torch.cuda.is_available():
            return "cuda"
        return "cpu"
    except:
        return "cpu"


def create_env(fast_mode: bool = False):
    """ìµœì í™”ëœ í™˜ê²½ ìƒì„±"""
    from config.scenario_config import ScenarioConfig
    from env.network_slicing_cmdp_env import NetworkSlicingCMDPEnv
    return NetworkSlicingCMDPEnv(config=ScenarioConfig())


def train(
    total_timesteps: int = 100000,
    device: str = "auto",
    fast_mode: bool = False,
    log_dir: str = "./logs",
    seed: int = 42
) -> Tuple[object, Dict]:
    """CMDP-SAC í•™ìŠµ ì‹¤í–‰"""
    
    # ì§€ì—° ì„í¬íŠ¸ (ì‹œì‘ ì‹œê°„ ë‹¨ì¶•)
    import torch
    from tqdm import tqdm
    from stable_baselines3 import SAC
    from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
    
    # í—¤ë” ì¶œë ¥
    print("\n" + "=" * 65)
    print("  ğŸš€ 5G O-RAN Network Slicing - CMDP-SAC Training")
    print("=" * 65)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if device == "auto":
        device = get_device()
    
    device_emoji = {"mps": "ğŸ", "cuda": "ğŸ®", "cpu": "ğŸ’»"}.get(device, "ğŸ’»")
    print(f"  {device_emoji} Device: {device.upper()}")
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(log_dir, f"run_{timestamp}")
    os.makedirs(run_dir, exist_ok=True)
    print(f"  ğŸ“ Log dir: {run_dir}")
    
    # í™˜ê²½ ìƒì„±
    print("  ğŸŒ Creating environment...", end=" ", flush=True)
    env = create_env(fast_mode)
    eval_env = create_env(fast_mode)
    print(f"Done (obs:{env.observation_space.shape[0]}, act:{env.action_space.shape[0]})")
    
    # SAC ëª¨ë¸
    print("  ğŸ§  Building SAC model...", end=" ", flush=True)
    
    policy_kwargs = {
        "net_arch": [256, 256] if fast_mode else [256, 256, 128],
        "activation_fn": torch.nn.ReLU
    }
    
    model = SAC(
        policy="MlpPolicy",
        env=env,
        learning_rate=3e-4,
        buffer_size=100000 if fast_mode else 200000,
        batch_size=256 if fast_mode else 512,
        gamma=0.99,
        tau=0.005,
        ent_coef="auto",
        learning_starts=1000 if fast_mode else 2000,
        train_freq=1,
        gradient_steps=1,
        policy_kwargs=policy_kwargs,
        device=device,
        seed=seed,
        verbose=0
    )
    print("Done")
    
    # tqdm ì½œë°±
    class ProgressCallback(BaseCallback):
        def __init__(self, total: int):
            super().__init__()
            self.total = total
            self.pbar = None
            self.metrics = {'profit': [], 'u_viol': [], 'e_viol': []}
            
        def _on_training_start(self):
            self.pbar = tqdm(
                total=self.total, desc="  ğŸ“ˆ Training", unit="step",
                ncols=100, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'
            )
            self.t0 = time.time()
            
        def _on_step(self) -> bool:
            self.pbar.update(1)
            if self.n_calls % 100 == 0:
                info = self.locals.get('infos', [{}])[0]
                for k, v in [('profit', 'profit'), ('u_viol', 'urllc_violation_rate'), ('e_viol', 'embb_violation_rate')]:
                    if v in info:
                        self.metrics[k].append(info[v])
                if self.metrics['profit']:
                    self.pbar.set_postfix({
                        'profit': f"${np.mean(self.metrics['profit'][-50:]):.0f}",
                        'U': f"{np.mean(self.metrics['u_viol'][-50:]):.3f}",
                        'E': f"{np.mean(self.metrics['e_viol'][-50:]):.3f}"
                    })
            return True
        
        def _on_training_end(self):
            self.pbar.close()
            print(f"  â±ï¸  Elapsed: {(time.time()-self.t0)/60:.1f} min")
    
    # ì½œë°± ì„¤ì •
    progress_cb = ProgressCallback(total_timesteps)
    checkpoint_cb = CheckpointCallback(
        save_freq=max(10000, total_timesteps // 5),
        save_path=run_dir, name_prefix="ckpt"
    )
    
    # ì„¤ì • ì €ì¥
    with open(os.path.join(run_dir, "config.json"), 'w') as f:
        json.dump({"timesteps": total_timesteps, "device": device, "fast": fast_mode, "seed": seed}, f)
    
    print(f"  ğŸ“Š Timesteps: {total_timesteps:,} (~{total_timesteps//168} episodes)")
    print("=" * 65 + "\n")
    
    # í•™ìŠµ
    start = time.time()
    try:
        model.learn(total_timesteps=total_timesteps, callback=[progress_cb, checkpoint_cb], progress_bar=False)
    except KeyboardInterrupt:
        print("\n  âš ï¸  Interrupted")
    
    # ì €ì¥
    model.save(os.path.join(run_dir, "final_model"))
    print(f"\n  ğŸ’¾ Model saved: {run_dir}/final_model")
    
    # í‰ê°€
    print("\n" + "=" * 65)
    print("  ğŸ“Š Evaluation (5 episodes)")
    print("=" * 65)
    
    results = evaluate(model, eval_env)
    
    with open(os.path.join(run_dir, "results.json"), 'w') as f:
        json.dump(results, f, indent=2)
    
    # ìš”ì•½
    elapsed = time.time() - start
    print(f"""
  â”Œ{'â”€'*61}â”
  â”‚  ğŸ“ˆ Mean Reward: {results['mean_reward']:>8.2f} Â± {results['std_reward']:.2f}{' '*20}â”‚
  â”‚  ğŸ’° Mean Profit: ${results['mean_profit']:>7.0f}{' '*32}â”‚
  â”‚  ğŸ”´ URLLC Viol:  {results['mean_urllc_violation']:>8.4f} (target: 0.001){' '*14}â”‚
  â”‚  ğŸ”µ eMBB Viol:   {results['mean_embb_violation']:>8.4f} (target: 0.01){' '*15}â”‚
  â”‚  âœ… Constraint:  {results['constraint_satisfaction']*100:>7.1f}%{' '*31}â”‚
  â”‚  â±ï¸  Total Time: {elapsed/60:>7.1f} min ({total_timesteps/elapsed:.0f} steps/sec){' '*10}â”‚
  â””{'â”€'*61}â”˜
""")
    
    return model, results


def evaluate(model, env, n_episodes: int = 5) -> Dict:
    """ëª¨ë¸ í‰ê°€"""
    from tqdm import trange
    
    rewards, profits, u_viols, e_viols = [], [], [], []
    
    for _ in trange(n_episodes, desc="  ğŸ” Eval", ncols=60):
        obs, _ = env.reset()
        done, ep_r, ep_p, ep_u, ep_e = False, 0, 0, [], []
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, r, term, trunc, info = env.step(action)
            done = term or trunc
            ep_r += r
            ep_p += info.get('profit', 0)
            ep_u.append(info.get('urllc_violation_rate', 0))
            ep_e.append(info.get('embb_violation_rate', 0))
        
        rewards.append(ep_r)
        profits.append(ep_p)
        u_viols.append(np.mean(ep_u))
        e_viols.append(np.mean(ep_e))
    
    sat = sum(1 for u, e in zip(u_viols, e_viols) if u <= 0.001 and e <= 0.01) / n_episodes
    
    return {
        "mean_reward": float(np.mean(rewards)),
        "std_reward": float(np.std(rewards)),
        "mean_profit": float(np.mean(profits)),
        "mean_urllc_violation": float(np.mean(u_viols)),
        "mean_embb_violation": float(np.mean(e_viols)),
        "constraint_satisfaction": float(sat)
    }


def main():
    parser = argparse.ArgumentParser(
        description="5G O-RAN CMDP-SAC Training (MPS Accelerated)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  í•œ ë²ˆì— ì‹¤í–‰ (One-liner):
  
    pip install -r requirements.txt && python run_training.py -t 100000 -f
    
  ì˜ˆì‹œ:
    python run_training.py                    # ê¸°ë³¸ (100K steps)
    python run_training.py -t 10000 -f        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10K)
    python run_training.py -t 500000          # ì „ì²´ í•™ìŠµ (500K)
    python run_training.py -d cpu             # CPU ê°•ì œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """
    )
    
    parser.add_argument("-t", "--timesteps", type=int, default=100000, help="í•™ìŠµ ìŠ¤í… (default: 100000)")
    parser.add_argument("-d", "--device", type=str, default="auto", choices=["auto", "mps", "cuda", "cpu"])
    parser.add_argument("-f", "--fast", action="store_true", help="ë¹ ë¥¸ ëª¨ë“œ (ë„¤íŠ¸ì›Œí¬ ì¶•ì†Œ)")
    parser.add_argument("-l", "--log-dir", type=str, default="./logs")
    parser.add_argument("-s", "--seed", type=int, default=42)
    
    args = parser.parse_args()
    
    train(
        total_timesteps=args.timesteps,
        device=args.device,
        fast_mode=args.fast,
        log_dir=args.log_dir,
        seed=args.seed
    )


if __name__ == "__main__":
    main()
